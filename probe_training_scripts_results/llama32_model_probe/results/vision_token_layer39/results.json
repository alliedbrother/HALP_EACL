{
  "model_name": "Llama-3.2-11B-Vision",
  "embedding_type": "vision_token_representation",
  "layer_name": "layer_39",
  "target_column": "is_hallucinating_manual",
  "input_dim": 4096,
  "timestamp": "20251003_190525",
  "dataset_statistics": {
    "total_samples": 9767,
    "num_no_hallucination": 8838,
    "num_hallucination": 929,
    "hallucination_percentage": 9.511620763796458
  },
  "train_set": {
    "num_samples": 7813,
    "accuracy": 0.9320363496736209,
    "precision": 0.8045977011494253,
    "recall": 0.3768506056527591,
    "f1": 0.5132905591200734,
    "auroc": 0.9629225910477992,
    "confusion_matrix": [
      [
        7002,
        68
      ],
      [
        463,
        280
      ]
    ],
    "num_no_hallucination": 7070,
    "num_hallucination": 743
  },
  "test_set": {
    "num_samples": 1954,
    "accuracy": 0.8904810644831116,
    "precision": 0.34782608695652173,
    "recall": 0.17204301075268819,
    "f1": 0.2302158273381295,
    "auroc": 0.7377481389578164,
    "confusion_matrix": [
      [
        1708,
        60
      ],
      [
        154,
        32
      ]
    ],
    "num_no_hallucination": 1768,
    "num_hallucination": 186
  },
  "training_history": {
    "train_loss": [
      0.31725082719812586,
      0.26831789180940513,
      0.24456348709610043,
      0.23699657795380574,
      0.22473367559058324,
      0.21143507282344662,
      0.20559958824697805,
      0.19612878919864188,
      0.198313437949638,
      0.18694267310962384,
      0.190303506778211,
      0.17950846356700878,
      0.1826682496587841,
      0.1808115605949139,
      0.17676745076872866,
      0.1714625274724498,
      0.16839962855589632,
      0.16734192071824658,
      0.16616142034834744,
      0.16676812860734608,
      0.1640505995905521,
      0.16883181505361383,
      0.1626294594942307,
      0.15923696446935742,
      0.15719176688972786,
      0.16324458972227818,
      0.1643259442278317,
      0.16435377109719781,
      0.1580268182772763,
      0.1574386160440591,
      0.1554206806938259,
      0.15561310209485948,
      0.160407498965458,
      0.15691004900299774,
      0.15480379354284735,
      0.1536508116034829,
      0.15137982134308134,
      0.1531807007686216,
      0.1505793114363843,
      0.14848450650365985,
      0.14653257814293005,
      0.14540837412433966,
      0.143144256211057,
      0.14900680553852294,
      0.14676815180145963,
      0.14528008538241288,
      0.1448701814235169,
      0.14640443492300656,
      0.14406699995149155,
      0.1469301298716847
    ],
    "train_acc": [
      0.89440675796749,
      0.904390119032382,
      0.9066939715858185,
      0.9077179060540126,
      0.906949955202867,
      0.9123256111608857,
      0.908485856905158,
      0.9125815947779342,
      0.9110456930756432,
      0.913477537437604,
      0.9115576603097402,
      0.9170613080762832,
      0.9165493408421861,
      0.9169333162677589,
      0.9171892998848074,
      0.9165493408421861,
      0.9202611032893895,
      0.920005119672341,
      0.9201331114808652,
      0.921029054140535,
      0.9207730705234864,
      0.9230769230769231,
      0.9248688083962626,
      0.9235888903110201,
      0.9253807756303597,
      0.9242288493536414,
      0.9244848329706898,
      0.9226929476513503,
      0.927044669141175,
      0.9255087674388839,
      0.9266606937156022,
      0.9239728657365929,
      0.9256367592474082,
      0.926020734672981,
      0.927044669141175,
      0.926020734672981,
      0.9266606937156022,
      0.926020734672981,
      0.9258927428644567,
      0.9257647510559324,
      0.9273006527582235,
      0.927556636375272,
      0.9261487264815051,
      0.926020734672981,
      0.9278126199923205,
      0.9273006527582235,
      0.9279406118008448,
      0.9288365544605145,
      0.9284525790349417,
      0.926020734672981
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/LLama_32/llama_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/llama32_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/llama32_model_probe/results/vision_token_layer39",
    "MODEL_NAME": "Llama-3.2-11B-Vision",
    "EMBEDDING_TYPE": "vision_token_representation",
    "LAYER_NAME": "layer_39",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}