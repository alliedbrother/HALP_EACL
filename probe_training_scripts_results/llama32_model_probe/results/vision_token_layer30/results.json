{
  "model_name": "Llama-3.2-11B-Vision",
  "embedding_type": "vision_token_representation",
  "layer_name": "layer_30",
  "target_column": "is_hallucinating_manual",
  "input_dim": 4096,
  "timestamp": "20251003_190434",
  "dataset_statistics": {
    "total_samples": 9767,
    "num_no_hallucination": 8838,
    "num_hallucination": 929,
    "hallucination_percentage": 9.511620763796458
  },
  "train_set": {
    "num_samples": 7813,
    "accuracy": 0.9316523742480481,
    "precision": 0.698292220113852,
    "recall": 0.4952893674293405,
    "f1": 0.5795275590551181,
    "auroc": 0.9609395375222967,
    "confusion_matrix": [
      [
        6911,
        159
      ],
      [
        375,
        368
      ]
    ],
    "num_no_hallucination": 7070,
    "num_hallucination": 743
  },
  "test_set": {
    "num_samples": 1954,
    "accuracy": 0.8807574206755373,
    "precision": 0.3333333333333333,
    "recall": 0.25268817204301075,
    "f1": 0.2874617737003058,
    "auroc": 0.759168369581083,
    "confusion_matrix": [
      [
        1674,
        94
      ],
      [
        139,
        47
      ]
    ],
    "num_no_hallucination": 1768,
    "num_hallucination": 186
  },
  "training_history": {
    "train_loss": [
      0.3266504483259454,
      0.26664914190769196,
      0.2511370983354899,
      0.2385057060694208,
      0.22671140029418224,
      0.21771644347784472,
      0.21144067678524522,
      0.21259434299201382,
      0.20149551404982197,
      0.20173639066669405,
      0.19601020548416645,
      0.19548681758496225,
      0.1871805011161736,
      0.1860195631123319,
      0.18343779332175547,
      0.1824260953737765,
      0.1826503082045487,
      0.1749970571118958,
      0.17787924390666338,
      0.16992044822902094,
      0.17083037687199457,
      0.172748114229465,
      0.17708578167521224,
      0.1703353719215612,
      0.16782944695073732,
      0.1657859012180445,
      0.1682147392508935,
      0.16271135142567206,
      0.16119019833146309,
      0.16153408071520378,
      0.16117904878088407,
      0.16321147815609466,
      0.15972254758282584,
      0.16014181786045736,
      0.15831824671857211,
      0.15972814661051546,
      0.15765736545531117,
      0.15269967823916553,
      0.15651651184853851,
      0.15479218394172434,
      0.1548073894211224,
      0.1514340057664988,
      0.15211140803080433,
      0.15307664838524498,
      0.15260175597302766,
      0.14882649277545967,
      0.14704576014560097,
      0.1497493812168131,
      0.15446656299945044,
      0.15059920263533688
    ],
    "train_acc": [
      0.8859592986048893,
      0.9041341354153334,
      0.9066939715858185,
      0.906949955202867,
      0.9055420453091002,
      0.9092538077563036,
      0.9124536029694099,
      0.9125815947779342,
      0.9132215538205555,
      0.9104057340330219,
      0.9125815947779342,
      0.9138615128631767,
      0.9153974145654679,
      0.9170613080762832,
      0.9148854473313708,
      0.9171892998848074,
      0.9177012671189044,
      0.9184692179700499,
      0.918981185204147,
      0.9207730705234864,
      0.9234608985024958,
      0.9196211442467682,
      0.9180852425444771,
      0.922564955842826,
      0.9221809804172533,
      0.9223089722257776,
      0.9202611032893895,
      0.9232049148854473,
      0.9234608985024958,
      0.9219249968002048,
      0.922564955842826,
      0.9246128247792141,
      0.9235888903110201,
      0.9234608985024958,
      0.9235888903110201,
      0.9248688083962626,
      0.9242288493536414,
      0.9273006527582235,
      0.926020734672981,
      0.9247408165877384,
      0.9256367592474082,
      0.9269166773326507,
      0.9265327019070779,
      0.9232049148854473,
      0.9248688083962626,
      0.9267886855241264,
      0.9261487264815051,
      0.9261487264815051,
      0.9298604889287085,
      0.9249968002047869
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/LLama_32/llama_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/llama32_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/llama32_model_probe/results/vision_token_layer30",
    "MODEL_NAME": "Llama-3.2-11B-Vision",
    "EMBEDDING_TYPE": "vision_token_representation",
    "LAYER_NAME": "layer_30",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}