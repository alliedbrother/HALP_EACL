{
  "model_name": "Llama-3.2-11B-Vision",
  "embedding_type": "vision_token_representation",
  "layer_name": "layer_0",
  "target_column": "is_hallucinating_manual",
  "input_dim": 4096,
  "timestamp": "20251003_190150",
  "dataset_statistics": {
    "total_samples": 9767,
    "num_no_hallucination": 8838,
    "num_hallucination": 929,
    "hallucination_percentage": 9.511620763796458
  },
  "train_set": {
    "num_samples": 7813,
    "accuracy": 0.9049020862664789,
    "precision": 0.0,
    "recall": 0.0,
    "f1": 0.0,
    "auroc": 0.5903719010624385,
    "confusion_matrix": [
      [
        7070,
        0
      ],
      [
        743,
        0
      ]
    ],
    "num_no_hallucination": 7070,
    "num_hallucination": 743
  },
  "test_set": {
    "num_samples": 1954,
    "accuracy": 0.9048106448311156,
    "precision": 0.0,
    "recall": 0.0,
    "f1": 0.0,
    "auroc": 0.5991187417895198,
    "confusion_matrix": [
      [
        1768,
        0
      ],
      [
        186,
        0
      ]
    ],
    "num_no_hallucination": 1768,
    "num_hallucination": 186
  },
  "training_history": {
    "train_loss": [
      0.36756533810070585,
      0.3191189856249459,
      0.3175519752563262,
      0.31991240570739826,
      0.3111626245841688,
      0.3112573103941217,
      0.3123406790653054,
      0.30839731538174103,
      0.31242325187337644,
      0.3127052117975391,
      0.3077201543413863,
      0.30575678254268607,
      0.31238995346487786,
      0.3098137373218731,
      0.3088381048367948,
      0.30675311109849385,
      0.3082251576440675,
      0.30611851425195225,
      0.3044663471835,
      0.3064708188176155,
      0.3086997902210878,
      0.3073618996204162,
      0.3048404058631586,
      0.3059596878837566,
      0.3074740325005687,
      0.30688910660695057,
      0.3067782587542826,
      0.30534517597787236,
      0.30506904216445224,
      0.3074574583951308,
      0.30470373584907884,
      0.3046386597411973,
      0.30251468918761426,
      0.306080341491164,
      0.30408796002062,
      0.3042351803609303,
      0.30712799965118875,
      0.3034782109211902,
      0.3062033552600413,
      0.30447407741935884,
      0.30320374448688664,
      0.304685264886642,
      0.3056143024138042,
      0.30565609862001575,
      0.30222373221601756,
      0.30634824855595216,
      0.3039867636804678,
      0.3055323122411358,
      0.3054130860433287,
      0.30492146954852706
    ],
    "train_acc": [
      0.8857033149878408,
      0.9017022910533726,
      0.9006783565851786,
      0.9025982337130424,
      0.9022142582874696,
      0.9018302828618968,
      0.9017022910533726,
      0.9024702419045181,
      0.9037501599897606,
      0.9033661845641879,
      0.9041341354153334,
      0.904390119032382,
      0.9038781517982849,
      0.9049020862664789,
      0.9041341354153334,
      0.9041341354153334,
      0.9045181108409062,
      0.9042621272238577,
      0.9047740944579546,
      0.9049020862664789,
      0.9037501599897606,
      0.904390119032382,
      0.9046461026494305,
      0.9046461026494305,
      0.9047740944579546,
      0.9045181108409062,
      0.9049020862664789,
      0.904390119032382,
      0.904390119032382,
      0.9042621272238577,
      0.9045181108409062,
      0.9047740944579546,
      0.9047740944579546,
      0.9049020862664789,
      0.9050300780750032,
      0.9049020862664789,
      0.904390119032382,
      0.9047740944579546,
      0.9046461026494305,
      0.9045181108409062,
      0.9045181108409062,
      0.9038781517982849,
      0.9047740944579546,
      0.9042621272238577,
      0.9045181108409062,
      0.9041341354153334,
      0.9046461026494305,
      0.9045181108409062,
      0.9045181108409062,
      0.9045181108409062
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/LLama_32/llama_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/llama32_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/llama32_model_probe/results/vision_token_layer0",
    "MODEL_NAME": "Llama-3.2-11B-Vision",
    "EMBEDDING_TYPE": "vision_token_representation",
    "LAYER_NAME": "layer_0",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}