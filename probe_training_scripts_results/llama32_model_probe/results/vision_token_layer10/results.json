{
  "model_name": "Llama-3.2-11B-Vision",
  "embedding_type": "vision_token_representation",
  "layer_name": "layer_10",
  "target_column": "is_hallucinating_manual",
  "input_dim": 4096,
  "timestamp": "20251003_190251",
  "dataset_statistics": {
    "total_samples": 9767,
    "num_no_hallucination": 8838,
    "num_hallucination": 929,
    "hallucination_percentage": 9.511620763796458
  },
  "train_set": {
    "num_samples": 7813,
    "accuracy": 0.9310124152054269,
    "precision": 0.7602040816326531,
    "recall": 0.40107671601615075,
    "f1": 0.5251101321585903,
    "auroc": 0.9561226801395771,
    "confusion_matrix": [
      [
        6976,
        94
      ],
      [
        445,
        298
      ]
    ],
    "num_no_hallucination": 7070,
    "num_hallucination": 743
  },
  "test_set": {
    "num_samples": 1954,
    "accuracy": 0.887410440122825,
    "precision": 0.33653846153846156,
    "recall": 0.1881720430107527,
    "f1": 0.2413793103448276,
    "auroc": 0.7463144066559626,
    "confusion_matrix": [
      [
        1699,
        69
      ],
      [
        151,
        35
      ]
    ],
    "num_no_hallucination": 1768,
    "num_hallucination": 186
  },
  "training_history": {
    "train_loss": [
      0.32283143246052215,
      0.27253930596064546,
      0.2573891693536116,
      0.24261569298651753,
      0.23725068027875862,
      0.23090233282775294,
      0.22083933770048375,
      0.21534594583268069,
      0.21940210145346972,
      0.20646128683369988,
      0.20518413925049256,
      0.19952393621206282,
      0.19714475287770739,
      0.19545181478772844,
      0.19242984657080806,
      0.2053590116756303,
      0.19597281778649409,
      0.1867406487464905,
      0.18681318358499177,
      0.1849224096354173,
      0.1844442008253263,
      0.18317652291485242,
      0.18381040030900314,
      0.18295682962147558,
      0.1761255003816011,
      0.1751066330592243,
      0.1784572939331434,
      0.17855686575782542,
      0.17299970459877229,
      0.17508795005174316,
      0.17321927523126407,
      0.16795465105346272,
      0.16939799753682955,
      0.1699598358145782,
      0.1684656573497519,
      0.16530513372652383,
      0.1677380281139393,
      0.1638007592789981,
      0.16612843709940814,
      0.1626920236692745,
      0.16593222656116194,
      0.1613463673246454,
      0.16240205161121427,
      0.16359943049978845,
      0.1630389379603522,
      0.1626066206973426,
      0.158047280711483,
      0.15882899349137228,
      0.15769496327456162,
      0.16264275917593313
    ],
    "train_acc": [
      0.8951747088186356,
      0.9052860616920517,
      0.9049020862664789,
      0.90643798796877,
      0.9097657749904006,
      0.9073339306284398,
      0.911941635735313,
      0.9124536029694099,
      0.9118136439267887,
      0.9120696275438372,
      0.9157813899910405,
      0.9157813899910405,
      0.915525406373992,
      0.9165493408421861,
      0.9150134391398951,
      0.9159093817995648,
      0.9168053244592346,
      0.9178292589274286,
      0.9157813899910405,
      0.9179572507359529,
      0.9171892998848074,
      0.9177012671189044,
      0.9198771278638167,
      0.920005119672341,
      0.9206450787149623,
      0.9212850377575835,
      0.9183412261615257,
      0.9216690131831563,
      0.9226929476513503,
      0.9224369640343018,
      0.9206450787149623,
      0.921029054140535,
      0.9211570459490592,
      0.9228209394598745,
      0.9233329066939716,
      0.9217970049916805,
      0.9207730705234864,
      0.9248688083962626,
      0.920517086906438,
      0.9248688083962626,
      0.9224369640343018,
      0.9241008575451171,
      0.9232049148854473,
      0.9244848329706898,
      0.9232049148854473,
      0.9242288493536414,
      0.9243568411621657,
      0.9242288493536414,
      0.926020734672981,
      0.9247408165877384
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/LLama_32/llama_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/llama32_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/llama32_model_probe/results/vision_token_layer10",
    "MODEL_NAME": "Llama-3.2-11B-Vision",
    "EMBEDDING_TYPE": "vision_token_representation",
    "LAYER_NAME": "layer_10",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}