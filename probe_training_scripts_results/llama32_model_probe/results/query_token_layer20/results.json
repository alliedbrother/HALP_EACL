{
  "model_name": "Llama-3.2-11B-Vision",
  "embedding_type": "query_token_representation",
  "layer_name": "layer_20",
  "target_column": "is_hallucinating_manual",
  "input_dim": 4096,
  "timestamp": "20251003_190809",
  "dataset_statistics": {
    "total_samples": 9767,
    "num_no_hallucination": 8838,
    "num_hallucination": 929,
    "hallucination_percentage": 9.511620763796458
  },
  "train_set": {
    "num_samples": 7813,
    "accuracy": 0.9769614744656342,
    "precision": 0.863225806451613,
    "recall": 0.9004037685060565,
    "f1": 0.8814229249011858,
    "auroc": 0.9959383286915501,
    "confusion_matrix": [
      [
        6964,
        106
      ],
      [
        74,
        669
      ]
    ],
    "num_no_hallucination": 7070,
    "num_hallucination": 743
  },
  "test_set": {
    "num_samples": 1954,
    "accuracy": 0.9165813715455476,
    "precision": 0.56353591160221,
    "recall": 0.5483870967741935,
    "f1": 0.555858310626703,
    "auroc": 0.9258502408407532,
    "confusion_matrix": [
      [
        1689,
        79
      ],
      [
        84,
        102
      ]
    ],
    "num_no_hallucination": 1768,
    "num_hallucination": 186
  },
  "training_history": {
    "train_loss": [
      0.2791363433611636,
      0.20816742796070722,
      0.1973764108912069,
      0.18822924918970282,
      0.18106029842581067,
      0.17672269701653598,
      0.17176377308003757,
      0.17055945438237824,
      0.16373356322244723,
      0.15566466244659863,
      0.14971871251354413,
      0.14863945612189722,
      0.14787633368859485,
      0.14109857391794117,
      0.1352066020044137,
      0.1356815677896446,
      0.13118631508277387,
      0.12937520909674313,
      0.12379702260071526,
      0.11996892022387105,
      0.12295817586080152,
      0.11885187133994637,
      0.11195861914632271,
      0.10951860230416059,
      0.1111919323122604,
      0.10995280514186134,
      0.10469092365582379,
      0.11101727794323649,
      0.10071241599222532,
      0.10279461903953735,
      0.11231861257574008,
      0.10039194675960711,
      0.09688087126301906,
      0.09205344680658713,
      0.09312000892752287,
      0.08418414658458181,
      0.0816421542867866,
      0.08923225100389777,
      0.08335394011715389,
      0.08450673053383219,
      0.0866781303678088,
      0.0825313495586113,
      0.08424288055156262,
      0.08567181748851221,
      0.07752816134835688,
      0.07935952209791511,
      0.07934147440163153,
      0.0824850590747534,
      0.07223655744835887,
      0.07693024932843995
    ],
    "train_acc": [
      0.8872392166901318,
      0.9073339306284398,
      0.9105337258415461,
      0.9133495456290798,
      0.9193651606297197,
      0.920517086906438,
      0.9233329066939716,
      0.9232049148854473,
      0.9247408165877384,
      0.9290925380775631,
      0.9299884807372328,
      0.9328043005247664,
      0.9289645462690388,
      0.9339562268014847,
      0.937156022014591,
      0.9386919237168821,
      0.9404838090362216,
      0.9389479073339306,
      0.942659669781134,
      0.9467554076539102,
      0.9447075387175221,
      0.9453474977601434,
      0.9482913093562012,
      0.9479073339306284,
      0.9484193011647255,
      0.9485472929732497,
      0.9522590554204531,
      0.9494432356329195,
      0.9514911045693075,
      0.9563547932932293,
      0.9475233585050556,
      0.9548188915909381,
      0.9523870472289774,
      0.9563547932932293,
      0.9576347113784718,
      0.9595545885063356,
      0.9616024574427237,
      0.9585306540381415,
      0.9609624984001024,
      0.9605785229745296,
      0.9618584410597721,
      0.9603225393574811,
      0.9644182772302573,
      0.9618584410597721,
      0.9660821707410726,
      0.965314219889927,
      0.9639063099961602,
      0.9649302444643543,
      0.965314219889927,
      0.961730449251248
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/LLama_32/llama_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/llama32_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/llama32_model_probe/results/query_token_layer20",
    "MODEL_NAME": "Llama-3.2-11B-Vision",
    "EMBEDDING_TYPE": "query_token_representation",
    "LAYER_NAME": "layer_20",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}