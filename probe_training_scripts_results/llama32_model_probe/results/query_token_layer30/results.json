{
  "model_name": "Llama-3.2-11B-Vision",
  "embedding_type": "query_token_representation",
  "layer_name": "layer_30",
  "target_column": "is_hallucinating_manual",
  "input_dim": 4096,
  "timestamp": "20251003_190900",
  "dataset_statistics": {
    "total_samples": 9767,
    "num_no_hallucination": 8838,
    "num_hallucination": 929,
    "hallucination_percentage": 9.511620763796458
  },
  "train_set": {
    "num_samples": 7813,
    "accuracy": 0.9801612696787405,
    "precision": 0.8909574468085106,
    "recall": 0.901749663526245,
    "f1": 0.8963210702341137,
    "auroc": 0.9969012052137728,
    "confusion_matrix": [
      [
        6988,
        82
      ],
      [
        73,
        670
      ]
    ],
    "num_no_hallucination": 7070,
    "num_hallucination": 743
  },
  "test_set": {
    "num_samples": 1954,
    "accuracy": 0.9211873080859775,
    "precision": 0.6111111111111112,
    "recall": 0.4731182795698925,
    "f1": 0.5333333333333333,
    "auroc": 0.9177978883861236,
    "confusion_matrix": [
      [
        1712,
        56
      ],
      [
        98,
        88
      ]
    ],
    "num_no_hallucination": 1768,
    "num_hallucination": 186
  },
  "training_history": {
    "train_loss": [
      0.2516099935283466,
      0.20668328970062488,
      0.19707738085060703,
      0.18961789018037367,
      0.18317037832980254,
      0.17731248584328865,
      0.16915688707816356,
      0.1681041135395668,
      0.1623243736825427,
      0.15552699327164768,
      0.15125966881002698,
      0.1431673730438461,
      0.14569386598863163,
      0.13606717170653296,
      0.13842239200460668,
      0.14514595501657043,
      0.13777738924385333,
      0.13214085135532885,
      0.12625523203489733,
      0.12804847406793612,
      0.12028780213120033,
      0.12196693874667494,
      0.12250399640096085,
      0.11987952484312106,
      0.11605345190179592,
      0.11342837896426114,
      0.11009429560247239,
      0.10733669232529569,
      0.10521528449821838,
      0.10883848241777444,
      0.10052457299767709,
      0.1062358811649741,
      0.1094596724349017,
      0.0973709225065398,
      0.09717185356825286,
      0.09286667451343252,
      0.09640827936937614,
      0.09473926990806145,
      0.09490881889716399,
      0.09222059892490506,
      0.0843146226777486,
      0.0899743356000708,
      0.08337932438399567,
      0.08424315029774242,
      0.09361816116285568,
      0.08460757326410741,
      0.08189474703477961,
      0.08280059354645865,
      0.07730889377203219,
      0.07956303018525394
    ],
    "train_acc": [
      0.9037501599897606,
      0.9104057340330219,
      0.9147574555228465,
      0.9173172916933316,
      0.9187252015870985,
      0.920517086906438,
      0.921541021374632,
      0.9219249968002048,
      0.9241008575451171,
      0.9266606937156022,
      0.9299884807372328,
      0.9348521694611545,
      0.933060284141815,
      0.9356201203123,
      0.9365160629719698,
      0.934084218610009,
      0.937156022014591,
      0.9393318827595034,
      0.9395878663765519,
      0.9395878663765519,
      0.9452195059516191,
      0.9438115960578523,
      0.9464994240368616,
      0.9444515551004735,
      0.9463714322283374,
      0.951235120952259,
      0.9518750799948803,
      0.9498272110584922,
      0.9493152438243952,
      0.949187252015871,
      0.949699219249968,
      0.9530270062715986,
      0.951235120952259,
      0.9564827851017534,
      0.9554588506335594,
      0.9575067195699475,
      0.956226801484705,
      0.9571227441443748,
      0.9555868424420837,
      0.9604505311660054,
      0.9599385639319084,
      0.9586586458466658,
      0.9610904902086267,
      0.9628823755279662,
      0.9618584410597721,
      0.961730449251248,
      0.9626263919109177,
      0.9619864328682964,
      0.9662101625495968,
      0.9618584410597721
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/LLama_32/llama_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/llama32_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/llama32_model_probe/results/query_token_layer30",
    "MODEL_NAME": "Llama-3.2-11B-Vision",
    "EMBEDDING_TYPE": "query_token_representation",
    "LAYER_NAME": "layer_30",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}