{
  "model_name": "Llama-3.2-11B-Vision",
  "embedding_type": "vision_token_representation",
  "layer_name": "layer_20",
  "target_column": "is_hallucinating_manual",
  "input_dim": 4096,
  "timestamp": "20251003_190343",
  "dataset_statistics": {
    "total_samples": 9767,
    "num_no_hallucination": 8838,
    "num_hallucination": 929,
    "hallucination_percentage": 9.511620763796458
  },
  "train_set": {
    "num_samples": 7813,
    "accuracy": 0.9315243824395238,
    "precision": 0.7321428571428571,
    "recall": 0.4414535666218035,
    "f1": 0.5507976490344249,
    "auroc": 0.9585687063226608,
    "confusion_matrix": [
      [
        6950,
        120
      ],
      [
        415,
        328
      ]
    ],
    "num_no_hallucination": 7070,
    "num_hallucination": 743
  },
  "test_set": {
    "num_samples": 1954,
    "accuracy": 0.8889457523029682,
    "precision": 0.36036036036036034,
    "recall": 0.21505376344086022,
    "f1": 0.26936026936026936,
    "auroc": 0.7391804116187417,
    "confusion_matrix": [
      [
        1697,
        71
      ],
      [
        146,
        40
      ]
    ],
    "num_no_hallucination": 1768,
    "num_hallucination": 186
  },
  "training_history": {
    "train_loss": [
      0.3298471096522954,
      0.26740979156932054,
      0.25467934866949005,
      0.24413868304418057,
      0.2348338771839531,
      0.22824773879683746,
      0.22604140770070408,
      0.21866195379775397,
      0.21453851403630508,
      0.21017468792139268,
      0.2033284945299431,
      0.2097096237296961,
      0.20387130148556767,
      0.1964611677490935,
      0.19535268545150758,
      0.19419180585109458,
      0.18739048616618526,
      0.19035041079837448,
      0.1864811284505591,
      0.1861217714389976,
      0.1796128094500425,
      0.18200025201147915,
      0.18523036462007736,
      0.1874536131565668,
      0.18097857682376492,
      0.17530852379847545,
      0.17509125431581418,
      0.17453366197189507,
      0.1685964086834265,
      0.1729986624754205,
      0.17348470903780996,
      0.16919605152643458,
      0.16914637769971574,
      0.16700341817067593,
      0.16891462995993847,
      0.16566160833653137,
      0.16654916361582522,
      0.1675009160777744,
      0.16183775899817748,
      0.16215155926742117,
      0.16008344006781675,
      0.16352273551177005,
      0.16224373027074093,
      0.15826069004073434,
      0.16350172768746105,
      0.1610492126369963,
      0.15841850326681622,
      0.1705881926326119,
      0.1568532284577282,
      0.15510011986658281
    ],
    "train_acc": [
      0.885831306796365,
      0.9051580698835274,
      0.9060540125431973,
      0.906949955202867,
      0.9086138487136823,
      0.9098937667989249,
      0.9133495456290798,
      0.9115576603097402,
      0.913989504671701,
      0.9132215538205555,
      0.9133495456290798,
      0.9148854473313708,
      0.913477537437604,
      0.9177012671189044,
      0.9171892998848074,
      0.9157813899910405,
      0.9188531933956227,
      0.9175732753103801,
      0.9197491360552925,
      0.9170613080762832,
      0.9212850377575835,
      0.9171892998848074,
      0.920517086906438,
      0.9220529886087291,
      0.920005119672341,
      0.9221809804172533,
      0.9194931524382439,
      0.918981185204147,
      0.9232049148854473,
      0.9198771278638167,
      0.9212850377575835,
      0.9220529886087291,
      0.9198771278638167,
      0.9234608985024958,
      0.9228209394598745,
      0.9207730705234864,
      0.9234608985024958,
      0.9239728657365929,
      0.9226929476513503,
      0.9235888903110201,
      0.9237168821195444,
      0.9220529886087291,
      0.921029054140535,
      0.9238448739280686,
      0.9243568411621657,
      0.9232049148854473,
      0.9251247920133111,
      0.9242288493536414,
      0.9258927428644567,
      0.9257647510559324
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/LLama_32/llama_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/llama32_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/llama32_model_probe/results/vision_token_layer20",
    "MODEL_NAME": "Llama-3.2-11B-Vision",
    "EMBEDDING_TYPE": "vision_token_representation",
    "LAYER_NAME": "layer_20",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}