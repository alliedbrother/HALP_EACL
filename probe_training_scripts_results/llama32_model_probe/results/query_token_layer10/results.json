{
  "model_name": "Llama-3.2-11B-Vision",
  "embedding_type": "query_token_representation",
  "layer_name": "layer_10",
  "target_column": "is_hallucinating_manual",
  "input_dim": 4096,
  "timestamp": "20251003_190717",
  "dataset_statistics": {
    "total_samples": 9767,
    "num_no_hallucination": 8838,
    "num_hallucination": 929,
    "hallucination_percentage": 9.511620763796458
  },
  "train_set": {
    "num_samples": 7813,
    "accuracy": 0.9580186868040446,
    "precision": 0.7603513174404015,
    "recall": 0.8156123822341858,
    "f1": 0.787012987012987,
    "auroc": 0.9845915008728329,
    "confusion_matrix": [
      [
        6879,
        191
      ],
      [
        137,
        606
      ]
    ],
    "num_no_hallucination": 7070,
    "num_hallucination": 743
  },
  "test_set": {
    "num_samples": 1954,
    "accuracy": 0.8863868986693961,
    "precision": 0.39655172413793105,
    "recall": 0.3709677419354839,
    "f1": 0.38333333333333336,
    "auroc": 0.8301403688026079,
    "confusion_matrix": [
      [
        1663,
        105
      ],
      [
        117,
        69
      ]
    ],
    "num_no_hallucination": 1768,
    "num_hallucination": 186
  },
  "training_history": {
    "train_loss": [
      0.31078962128685444,
      0.2511155458737393,
      0.24135873264500074,
      0.23292961835252995,
      0.2269728126574536,
      0.22238625224147524,
      0.21205906011924452,
      0.21930907313921014,
      0.20764060932762768,
      0.1990221030553993,
      0.19760591430323465,
      0.1900375699814485,
      0.18287522718310356,
      0.18526486792430585,
      0.1808482221650834,
      0.17957402676039813,
      0.19003722333178227,
      0.1704597620103432,
      0.17231150854911123,
      0.16609157845377923,
      0.16193908392166606,
      0.16283973498003823,
      0.1565127257029621,
      0.15015105355378924,
      0.15218832984718741,
      0.15171301305598142,
      0.14184775570673602,
      0.145002457060452,
      0.14363741123554657,
      0.140128797216683,
      0.1416872673078763,
      0.13561898544430734,
      0.13616082522333886,
      0.13501118457286942,
      0.13299720917581295,
      0.13280659233581046,
      0.12596134147321691,
      0.12535227350130373,
      0.12086998478691237,
      0.12191898094452157,
      0.12008215398052517,
      0.11736385265213191,
      0.12370220169577063,
      0.12234678443293182,
      0.12250088948224272,
      0.120463773767863,
      0.10970508303654818,
      0.11090211877409292,
      0.10740543739604098,
      0.11761268401632503
    ],
    "train_acc": [
      0.8863432740304621,
      0.9079738896710611,
      0.9083578650966339,
      0.9081018814795853,
      0.9115576603097402,
      0.9107897094585946,
      0.9127095865864585,
      0.9130935620120312,
      0.9128375783949827,
      0.9141174964802252,
      0.9182132343530014,
      0.9188531933956227,
      0.9165493408421861,
      0.9194931524382439,
      0.9216690131831563,
      0.9212850377575835,
      0.9170613080762832,
      0.9238448739280686,
      0.9232049148854473,
      0.9257647510559324,
      0.9252527838218354,
      0.9257647510559324,
      0.9281965954178932,
      0.9292205298860873,
      0.9319083578650966,
      0.9294765135031358,
      0.9367720465890182,
      0.9312683988224754,
      0.934084218610009,
      0.9362600793549213,
      0.9356201203123,
      0.9363880711634456,
      0.9352361448867272,
      0.9369000383975425,
      0.9386919237168821,
      0.9370280302060668,
      0.9415077435044157,
      0.9418917189299885,
      0.9438115960578523,
      0.9429156533981825,
      0.9435556124408038,
      0.9438115960578523,
      0.9411237680788429,
      0.9456034813771919,
      0.942147702547037,
      0.9459874568027646,
      0.9488032765902982,
      0.9500831946755408,
      0.9504671701011135,
      0.942147702547037
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/LLama_32/llama_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/llama32_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/llama32_model_probe/results/query_token_layer10",
    "MODEL_NAME": "Llama-3.2-11B-Vision",
    "EMBEDDING_TYPE": "query_token_representation",
    "LAYER_NAME": "layer_10",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}