{
  "model_name": "Llama-3.2-11B-Vision",
  "embedding_type": "query_token_representation",
  "layer_name": "layer_39",
  "target_column": "is_hallucinating_manual",
  "input_dim": 4096,
  "timestamp": "20251003_190952",
  "dataset_statistics": {
    "total_samples": 9767,
    "num_no_hallucination": 8838,
    "num_hallucination": 929,
    "hallucination_percentage": 9.511620763796458
  },
  "train_set": {
    "num_samples": 7813,
    "accuracy": 0.9808012287213619,
    "precision": 0.9001349527665317,
    "recall": 0.8977119784656796,
    "f1": 0.898921832884097,
    "auroc": 0.9968758863965612,
    "confusion_matrix": [
      [
        6996,
        74
      ],
      [
        76,
        667
      ]
    ],
    "num_no_hallucination": 7070,
    "num_hallucination": 743
  },
  "test_set": {
    "num_samples": 1954,
    "accuracy": 0.9119754350051177,
    "precision": 0.5472972972972973,
    "recall": 0.43548387096774194,
    "f1": 0.48502994011976047,
    "auroc": 0.8959367245657568,
    "confusion_matrix": [
      [
        1701,
        67
      ],
      [
        105,
        81
      ]
    ],
    "num_no_hallucination": 1768,
    "num_hallucination": 186
  },
  "training_history": {
    "train_loss": [
      0.27539872046636077,
      0.2096011253644009,
      0.19073098713950234,
      0.182527992250968,
      0.18127094726173246,
      0.16751706460300758,
      0.16208586138578093,
      0.1546319603463825,
      0.14838966404905124,
      0.14905535762711447,
      0.13564600789425324,
      0.13192660264024625,
      0.12949993827057127,
      0.13239276077674358,
      0.12021581817190258,
      0.11616042771813821,
      0.11733184559175706,
      0.11517578415876749,
      0.10668260333679465,
      0.10344010204532925,
      0.09892258943267623,
      0.10056314889569672,
      0.09769535874947906,
      0.09462358480775539,
      0.09405013665328829,
      0.09205820395834553,
      0.08570034452618994,
      0.08933597276633491,
      0.08324219487151321,
      0.089895956894877,
      0.0829171566273637,
      0.08231541945418457,
      0.07965544272328214,
      0.07370041148946145,
      0.07320586378325004,
      0.07804207965893177,
      0.08277477047178058,
      0.0786709724716386,
      0.06604121283732582,
      0.061185273998036825,
      0.06854267390207293,
      0.0628713443214834,
      0.07213079555207218,
      0.057581604437484425,
      0.05946009356471501,
      0.05778785918848779,
      0.05865712254925878,
      0.06493354723266116,
      0.07009379651245413,
      0.054200514478428406
    ],
    "train_acc": [
      0.8885191347753744,
      0.9107897094585946,
      0.9146294637143223,
      0.9188531933956227,
      0.9191091770126711,
      0.9216690131831563,
      0.9238448739280686,
      0.928068603609369,
      0.9370280302060668,
      0.9333162677588634,
      0.9351081530782029,
      0.9365160629719698,
      0.94163573531294,
      0.9411237680788429,
      0.9447075387175221,
      0.9468833994624344,
      0.9461154486112888,
      0.9472673748880072,
      0.9521310636119288,
      0.9513631127607833,
      0.9552028670165109,
      0.9518750799948803,
      0.9539229489312684,
      0.9563547932932293,
      0.9585306540381415,
      0.9580186868040446,
      0.9612184820171509,
      0.9563547932932293,
      0.9645462690387815,
      0.959810572123384,
      0.9654422116984513,
      0.9626263919109177,
      0.963266350953539,
      0.9682580314859849,
      0.9678740560604121,
      0.9635223345705874,
      0.9668501215922181,
      0.9650582362728786,
      0.9708178676564699,
      0.9722257775502368,
      0.9686420069115577,
      0.972865736592858,
      0.9682580314859849,
      0.9755535645718674,
      0.9727377447843338,
      0.9740176628695764,
      0.9741456546781005,
      0.972865736592858,
      0.9682580314859849,
      0.9751695891462946
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/LLama_32/llama_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/llama32_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/llama32_model_probe/results/query_token_layer39",
    "MODEL_NAME": "Llama-3.2-11B-Vision",
    "EMBEDDING_TYPE": "query_token_representation",
    "LAYER_NAME": "layer_39",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}