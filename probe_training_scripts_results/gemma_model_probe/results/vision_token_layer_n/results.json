{
  "model_name": "Gemma3-12B",
  "embedding_type": "vision_token_representation",
  "layer_name": "layer_47",
  "target_column": "is_hallucinating_manual",
  "input_dim": 3840,
  "timestamp": "20251003_170026",
  "dataset_statistics": {
    "total_samples": 10000,
    "num_no_hallucination": 8942,
    "num_hallucination": 1058,
    "hallucination_percentage": 10.58
  },
  "train_set": {
    "num_samples": 8000,
    "accuracy": 0.93575,
    "precision": 0.8192307692307692,
    "recall": 0.5035460992907801,
    "f1": 0.623718887262079,
    "auroc": 0.9235142468529236,
    "confusion_matrix": [
      [
        7060,
        94
      ],
      [
        420,
        426
      ]
    ],
    "num_no_hallucination": 7154,
    "num_hallucination": 846
  },
  "test_set": {
    "num_samples": 2000,
    "accuracy": 0.8635,
    "precision": 0.23478260869565218,
    "recall": 0.12735849056603774,
    "f1": 0.1651376146788991,
    "auroc": 0.5955716305770125,
    "confusion_matrix": [
      [
        1700,
        88
      ],
      [
        185,
        27
      ]
    ],
    "num_no_hallucination": 1788,
    "num_hallucination": 212
  },
  "training_history": {
    "train_loss": [
      0.5103790640234948,
      0.34459794709086417,
      0.30319098058342936,
      0.27655181649327276,
      0.24615185809135437,
      0.22656506890058517,
      0.2120069136917591,
      0.19249772350490094,
      0.18993632112443448,
      0.18665695491433143,
      0.17474345952272416,
      0.16831331661343574,
      0.16036957985162734,
      0.16020775333046913,
      0.15410769081115722,
      0.15235932752490045,
      0.1541452538073063,
      0.14884864699840544,
      0.14806035823374986,
      0.1378847184702754,
      0.13910627005249263,
      0.1399152453839779,
      0.13557272440195084,
      0.13242437028884887,
      0.13202016025781632,
      0.13463952251523734,
      0.13707185883820056,
      0.13213607785850764,
      0.1320283192023635,
      0.12715286191552877,
      0.1211764667108655,
      0.13076052034646274,
      0.1280700190216303,
      0.12692144430428742,
      0.12601829345524312,
      0.1259458864927292,
      0.127892112955451,
      0.1252210804373026,
      0.12374314442276954,
      0.12629887802153825,
      0.11989463747292757,
      0.12391976344585419,
      0.12167770013213158,
      0.12300226953625679,
      0.11816546312719584,
      0.11963273838162422,
      0.12295783685147763,
      0.11535297336429358,
      0.11839822052046657,
      0.11516223098337651
    ],
    "train_acc": [
      0.783875,
      0.88475,
      0.888625,
      0.89175,
      0.897625,
      0.901,
      0.906125,
      0.909625,
      0.9125,
      0.90975,
      0.9185,
      0.91925,
      0.918,
      0.921375,
      0.925,
      0.924125,
      0.92575,
      0.924375,
      0.926625,
      0.931,
      0.928875,
      0.928875,
      0.931125,
      0.931375,
      0.92825,
      0.93225,
      0.928375,
      0.93225,
      0.9305,
      0.931375,
      0.9355,
      0.930375,
      0.93425,
      0.93375,
      0.936125,
      0.935,
      0.933125,
      0.930875,
      0.932375,
      0.935,
      0.93475,
      0.931625,
      0.935625,
      0.933625,
      0.935125,
      0.936125,
      0.934125,
      0.93575,
      0.93325,
      0.936
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/Gemma3_12B/gemma_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/gemma3_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/gemma_model_probe/results/vision_token_layer_n",
    "MODEL_NAME": "Gemma3-12B",
    "EMBEDDING_TYPE": "vision_token_representation",
    "LAYER_NAME": "layer_47",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}