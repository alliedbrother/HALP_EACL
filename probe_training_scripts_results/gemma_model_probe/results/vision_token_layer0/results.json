{
  "model_name": "Gemma3-12B",
  "embedding_type": "vision_token_representation",
  "layer_name": "layer_0",
  "target_column": "is_hallucinating_manual",
  "input_dim": 3840,
  "timestamp": "20251003_165446",
  "dataset_statistics": {
    "total_samples": 10000,
    "num_no_hallucination": 8942,
    "num_hallucination": 1058,
    "hallucination_percentage": 10.58
  },
  "train_set": {
    "num_samples": 8000,
    "accuracy": 0.928375,
    "precision": 0.7052631578947368,
    "recall": 0.5543735224586288,
    "f1": 0.6207809397749835,
    "auroc": 0.9573871616070893,
    "confusion_matrix": [
      [
        6958,
        196
      ],
      [
        377,
        469
      ]
    ],
    "num_no_hallucination": 7154,
    "num_hallucination": 846
  },
  "test_set": {
    "num_samples": 2000,
    "accuracy": 0.8585,
    "precision": 0.24822695035460993,
    "recall": 0.1650943396226415,
    "f1": 0.19830028328611898,
    "auroc": 0.6538427039804146,
    "confusion_matrix": [
      [
        1682,
        106
      ],
      [
        177,
        35
      ]
    ],
    "num_no_hallucination": 1788,
    "num_hallucination": 212
  },
  "training_history": {
    "train_loss": [
      0.486706431388855,
      0.33661869859695437,
      0.3187335359454155,
      0.3039161266088486,
      0.296227241396904,
      0.2840950399637222,
      0.27542378798127176,
      0.2643145786821842,
      0.26105884334445,
      0.258556153267622,
      0.2483580458164215,
      0.24609534406661987,
      0.24209400859475136,
      0.23925972354412078,
      0.23392844545841218,
      0.22487206482887268,
      0.2215791906416416,
      0.22070128491520882,
      0.210783030629158,
      0.2151165478825569,
      0.2209621057808399,
      0.20821033626794816,
      0.20893594878911972,
      0.20461756220459937,
      0.2003969496637583,
      0.20274542325735093,
      0.20328300288319587,
      0.19626337882876396,
      0.1961305858194828,
      0.19135175389051437,
      0.1943819536268711,
      0.1863067507147789,
      0.19375455871224403,
      0.19171103736758233,
      0.18969656771421434,
      0.18226334957778453,
      0.1883329517543316,
      0.18696303486824037,
      0.18572338618338108,
      0.1855951317101717,
      0.1893465076982975,
      0.18242878521978856,
      0.18558763234317302,
      0.18536511716246604,
      0.17700531242787837,
      0.1787214068621397,
      0.1782724671959877,
      0.17688626196980475,
      0.17782103969156743,
      0.17586894330382347
    ],
    "train_acc": [
      0.79825,
      0.8875,
      0.891,
      0.893875,
      0.892875,
      0.893,
      0.894375,
      0.89375,
      0.898125,
      0.895875,
      0.8975,
      0.89675,
      0.90025,
      0.900625,
      0.900875,
      0.902375,
      0.9,
      0.90225,
      0.907,
      0.906125,
      0.901,
      0.908875,
      0.90875,
      0.91025,
      0.912625,
      0.906875,
      0.90975,
      0.913375,
      0.911625,
      0.912625,
      0.91175,
      0.915,
      0.909625,
      0.911,
      0.91225,
      0.914625,
      0.913875,
      0.913,
      0.915125,
      0.913125,
      0.914125,
      0.916375,
      0.914625,
      0.913625,
      0.917625,
      0.916,
      0.916375,
      0.918375,
      0.91825,
      0.91825
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/Gemma3_12B/gemma_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/gemma3_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/gemma_model_probe/results/vision_token_layer0",
    "MODEL_NAME": "Gemma3-12B",
    "EMBEDDING_TYPE": "vision_token_representation",
    "LAYER_NAME": "layer_0",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}