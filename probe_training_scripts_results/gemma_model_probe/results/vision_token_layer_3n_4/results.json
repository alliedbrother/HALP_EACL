{
  "model_name": "Gemma3-12B",
  "embedding_type": "vision_token_representation",
  "layer_name": "layer_36",
  "target_column": "is_hallucinating_manual",
  "input_dim": 3840,
  "timestamp": "20251003_165920",
  "dataset_statistics": {
    "total_samples": 10000,
    "num_no_hallucination": 8942,
    "num_hallucination": 1058,
    "hallucination_percentage": 10.58
  },
  "train_set": {
    "num_samples": 8000,
    "accuracy": 0.937375,
    "precision": 0.8273244781783681,
    "recall": 0.5153664302600472,
    "f1": 0.6351056081573198,
    "auroc": 0.969016242463176,
    "confusion_matrix": [
      [
        7063,
        91
      ],
      [
        410,
        436
      ]
    ],
    "num_no_hallucination": 7154,
    "num_hallucination": 846
  },
  "test_set": {
    "num_samples": 2000,
    "accuracy": 0.863,
    "precision": 0.21818181818181817,
    "recall": 0.11320754716981132,
    "f1": 0.14906832298136646,
    "auroc": 0.5994694715292728,
    "confusion_matrix": [
      [
        1702,
        86
      ],
      [
        188,
        24
      ]
    ],
    "num_no_hallucination": 1788,
    "num_hallucination": 212
  },
  "training_history": {
    "train_loss": [
      0.5081608047485352,
      0.3437354298830032,
      0.3072946729362011,
      0.2828933161795139,
      0.2574592798054218,
      0.2333301212489605,
      0.223159304022789,
      0.20853143545985223,
      0.20079927083849908,
      0.19053508812189102,
      0.18612099570035934,
      0.1762685819864273,
      0.17318194843828677,
      0.16915077622234823,
      0.15890570963919162,
      0.1622866335362196,
      0.1631305418908596,
      0.15485075931996106,
      0.15232952307909728,
      0.14654564798623324,
      0.14369969272613525,
      0.14136286845803261,
      0.14049814861267806,
      0.14371769960224628,
      0.14079785777628423,
      0.13770603892207145,
      0.1336790194362402,
      0.13596008708328008,
      0.13732506985217333,
      0.13119687017798423,
      0.13070279435813428,
      0.133345087364316,
      0.13147578300535678,
      0.12737769127637147,
      0.12945873650163411,
      0.1252695794403553,
      0.1291168556958437,
      0.1238939556851983,
      0.12448367750644684,
      0.12383356456086039,
      0.12255888719111681,
      0.12125381219387055,
      0.1270929508879781,
      0.12366290266811848,
      0.1185706269480288,
      0.1202405793145299,
      0.11930665651708841,
      0.12299939914792776,
      0.11518387161195279,
      0.12051074808835983
    ],
    "train_acc": [
      0.77825,
      0.88425,
      0.8905,
      0.895625,
      0.895125,
      0.901125,
      0.902125,
      0.906875,
      0.907375,
      0.913875,
      0.9145,
      0.917,
      0.91675,
      0.92175,
      0.922,
      0.919625,
      0.91975,
      0.922125,
      0.924625,
      0.924,
      0.926875,
      0.92775,
      0.930125,
      0.92775,
      0.928,
      0.931625,
      0.92875,
      0.92825,
      0.9305,
      0.931,
      0.929,
      0.92975,
      0.93225,
      0.932125,
      0.930875,
      0.930875,
      0.93175,
      0.930875,
      0.9315,
      0.931375,
      0.93125,
      0.933625,
      0.932375,
      0.9335,
      0.933875,
      0.933875,
      0.935125,
      0.93125,
      0.939125,
      0.932375
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/Gemma3_12B/gemma_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/gemma3_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/gemma_model_probe/results/vision_token_layer_3n_4",
    "MODEL_NAME": "Gemma3-12B",
    "EMBEDDING_TYPE": "vision_token_representation",
    "LAYER_NAME": "layer_36",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}