{
  "model_name": "Gemma3-12B",
  "embedding_type": "vision_token_representation",
  "layer_name": "layer_24",
  "target_column": "is_hallucinating_manual",
  "input_dim": 3840,
  "timestamp": "20251003_165821",
  "dataset_statistics": {
    "total_samples": 10000,
    "num_no_hallucination": 8942,
    "num_hallucination": 1058,
    "hallucination_percentage": 10.58
  },
  "train_set": {
    "num_samples": 8000,
    "accuracy": 0.93725,
    "precision": 0.7819672131147541,
    "recall": 0.5638297872340425,
    "f1": 0.6552197802197802,
    "auroc": 0.9692870493189018,
    "confusion_matrix": [
      [
        7021,
        133
      ],
      [
        369,
        477
      ]
    ],
    "num_no_hallucination": 7154,
    "num_hallucination": 846
  },
  "test_set": {
    "num_samples": 2000,
    "accuracy": 0.854,
    "precision": 0.23684210526315788,
    "recall": 0.16981132075471697,
    "f1": 0.1978021978021978,
    "auroc": 0.6146281815879447,
    "confusion_matrix": [
      [
        1672,
        116
      ],
      [
        176,
        36
      ]
    ],
    "num_no_hallucination": 1788,
    "num_hallucination": 212
  },
  "training_history": {
    "train_loss": [
      0.5082357274293899,
      0.34219970771670344,
      0.3203244253396988,
      0.2984176478981972,
      0.28091876980662345,
      0.2660077295601368,
      0.25462662225961685,
      0.24237286841869354,
      0.23096010702848435,
      0.22301368641853334,
      0.21537770810723306,
      0.21014154875278473,
      0.20848101982474326,
      0.20179891830682756,
      0.19016472235322,
      0.18669322608411312,
      0.1810740770101547,
      0.17784608246386052,
      0.1791812261044979,
      0.17497571992874145,
      0.16934274879097938,
      0.16305391882359982,
      0.16139081315696238,
      0.1547613974362612,
      0.1565968841612339,
      0.1579170633852482,
      0.1509429072290659,
      0.1503620982915163,
      0.15986190040409565,
      0.15278011390566826,
      0.15050190730392932,
      0.14354199039936066,
      0.14731960044801234,
      0.14707217916846274,
      0.13949551833420992,
      0.1440008090659976,
      0.1409033136740327,
      0.14136209498345853,
      0.14496889965981244,
      0.13735796996206046,
      0.14440397174656391,
      0.13419106408953665,
      0.13762580643594266,
      0.13419471364468336,
      0.13637918394058943,
      0.13605197471380234,
      0.13485910435020923,
      0.13248691514879465,
      0.1304254019148648,
      0.1314671578258276
    ],
    "train_acc": [
      0.77625,
      0.89,
      0.893125,
      0.894125,
      0.89625,
      0.897875,
      0.89925,
      0.902625,
      0.9025,
      0.90475,
      0.90425,
      0.905,
      0.907,
      0.90675,
      0.9115,
      0.913625,
      0.914,
      0.91625,
      0.915375,
      0.916125,
      0.921375,
      0.921125,
      0.9225,
      0.924125,
      0.92325,
      0.921625,
      0.924875,
      0.927125,
      0.922375,
      0.923625,
      0.924375,
      0.92225,
      0.924625,
      0.927,
      0.929625,
      0.92825,
      0.92825,
      0.925,
      0.9255,
      0.92775,
      0.9265,
      0.931125,
      0.9285,
      0.928875,
      0.928375,
      0.92675,
      0.93025,
      0.929125,
      0.92925,
      0.92875
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/Gemma3_12B/gemma_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/gemma3_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/gemma_model_probe/results/vision_token_layer_n_2",
    "MODEL_NAME": "Gemma3-12B",
    "EMBEDDING_TYPE": "vision_token_representation",
    "LAYER_NAME": "layer_24",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}