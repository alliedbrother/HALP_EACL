{
  "model_name": "SmolVLM2-2.2B",
  "embedding_type": "query_token_representation",
  "layer_name": "layer_23",
  "target_column": "is_hallucinating_manual",
  "input_dim": 2048,
  "timestamp": "20251004_120728",
  "dataset_statistics": {
    "total_samples": 10000,
    "num_no_hallucination": 9009,
    "num_hallucination": 991,
    "hallucination_percentage": 9.91
  },
  "train_set": {
    "num_samples": 8000,
    "accuracy": 0.96975,
    "precision": 0.833939393939394,
    "recall": 0.8675914249684742,
    "f1": 0.8504326328800988,
    "auroc": 0.9927452485507382,
    "confusion_matrix": [
      [
        7070,
        137
      ],
      [
        105,
        688
      ]
    ],
    "num_no_hallucination": 7207,
    "num_hallucination": 793
  },
  "test_set": {
    "num_samples": 2000,
    "accuracy": 0.9045,
    "precision": 0.5152838427947598,
    "recall": 0.5959595959595959,
    "f1": 0.5526932084309133,
    "auroc": 0.901446484826063,
    "confusion_matrix": [
      [
        1691,
        111
      ],
      [
        80,
        118
      ]
    ],
    "num_no_hallucination": 1802,
    "num_hallucination": 198
  },
  "training_history": {
    "train_loss": [
      0.4388610765337944,
      0.2256593124270439,
      0.20001552277803422,
      0.18118215264379978,
      0.1707154196202755,
      0.17072581224143504,
      0.15387865929305553,
      0.15755499141663312,
      0.1499108350351453,
      0.14858656834065914,
      0.1378858411386609,
      0.13145036858320236,
      0.13973400662094354,
      0.1308634593859315,
      0.13118855264782905,
      0.12258220901340246,
      0.12190938258171082,
      0.11840672652423381,
      0.11862871010601521,
      0.11602955412864685,
      0.11207291819900274,
      0.11133546203374863,
      0.1131303142644465,
      0.10802417151629924,
      0.1068539427369833,
      0.10293586361408234,
      0.09814686395227909,
      0.10215306111425161,
      0.09760656715929508,
      0.09694256391562521,
      0.10327496189251542,
      0.09470379039086402,
      0.09044938603416085,
      0.08900144759565592,
      0.08848453709110618,
      0.09099687176477164,
      0.08778333145007491,
      0.08783777585625649,
      0.08657709914818407,
      0.0856181273907423,
      0.08186807418242097,
      0.08770020555891096,
      0.0850008343718946,
      0.08040989422611892,
      0.08646996982954443,
      0.08072890833066776,
      0.0790211292039603,
      0.07941935990005732,
      0.07387080112658441,
      0.0749534911159426
    ],
    "train_acc": [
      0.814375,
      0.90975,
      0.917625,
      0.9245,
      0.9265,
      0.925,
      0.9315,
      0.932,
      0.93325,
      0.9335,
      0.936625,
      0.940125,
      0.937125,
      0.939625,
      0.93925,
      0.9455,
      0.946625,
      0.9445,
      0.944,
      0.9495,
      0.946875,
      0.94825,
      0.95,
      0.951125,
      0.951375,
      0.95375,
      0.952625,
      0.952625,
      0.954625,
      0.955,
      0.954125,
      0.958,
      0.95975,
      0.957625,
      0.959375,
      0.959375,
      0.958,
      0.9585,
      0.96125,
      0.9615,
      0.96175,
      0.959625,
      0.961125,
      0.964375,
      0.961375,
      0.9615,
      0.963,
      0.964125,
      0.96825,
      0.9655
    ]
  },
  "config": {
    "H5_DIR": "/root/akhil/HALP_EACL_Models/Models/Smol_VL/smolvlm_output",
    "CSV_PATH": "/root/akhil/FInal_CSV_Hallucination/smolvlm_manually_reviewed.csv",
    "OUTPUT_DIR": "/root/akhil/probe_training_scripts/smolvlm_model_probe/results/query_token_layer23",
    "MODEL_NAME": "SmolVLM2-2.2B",
    "EMBEDDING_TYPE": "query_token_representation",
    "LAYER_NAME": "layer_23",
    "TARGET_COLUMN": "is_hallucinating_manual",
    "LAYER_SIZES": [
      512,
      256,
      128
    ],
    "DROPOUT_RATE": 0.3,
    "LEARNING_RATE": 0.001,
    "BATCH_SIZE": 32,
    "EPOCHS": 50,
    "TEST_SIZE": 0.2,
    "RANDOM_STATE": 42,
    "DEVICE": "cuda"
  }
}