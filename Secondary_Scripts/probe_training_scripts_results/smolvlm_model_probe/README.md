# SmolVLM2-2.2B Hallucination Detection Probe Training

This directory contains all scripts for training hallucination detection probes on SmolVLM2-2.2B embeddings.

## ğŸ“ Directory Structure

```
smolvlm_model_probe/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ verify_setup.py                     # Pre-flight verification script
â”œâ”€â”€ run_all_probes.py                   # Master execution script (Python)
â”œâ”€â”€ run_all_probes.sh                   # Master execution script (Bash)
â”œâ”€â”€ generate_all_probe_scripts.py       # Script generator (already executed)
â”‚
â”œâ”€â”€ 01_vision_only_probe.py             # Vision encoder probe
â”‚
â”œâ”€â”€ 02_vision_token_layer0_probe.py     # Vision token layer 0
â”œâ”€â”€ 03_vision_token_layer6_probe.py     # Vision token layer 6 (n/4)
â”œâ”€â”€ 04_vision_token_layer12_probe.py    # Vision token layer 12 (n/2)
â”œâ”€â”€ 05_vision_token_layer18_probe.py    # Vision token layer 18 (3n/4)
â”œâ”€â”€ 06_vision_token_layer23_probe.py    # Vision token layer 23 (final)
â”‚
â”œâ”€â”€ 07_query_token_layer0_probe.py      # Query token layer 0
â”œâ”€â”€ 08_query_token_layer6_probe.py      # Query token layer 6 (n/4)
â”œâ”€â”€ 09_query_token_layer12_probe.py     # Query token layer 12 (n/2)
â”œâ”€â”€ 10_query_token_layer18_probe.py     # Query token layer 18 (3n/4)
â””â”€â”€ 11_query_token_layer23_probe.py     # Query token layer 23 (final)
```

## ğŸ¯ Probe Configuration

**Model:** SmolVLM2-2.2B-Instruct
**Total Layers:** 24 text decoder layers
**Selected Layers:** [0, 6, 12, 18, 23]

### Embeddings

- **Vision Only:** (1152,) - Vision encoder output
- **Vision Token:** (2048,) - Vision token at last image position
- **Query Token:** (2048,) - Query token at last question position

### Training Configuration

- **Architecture:** [512, 256, 128] â†’ 1 (Binary classifier)
- **Dropout:** 0.3
- **Learning Rate:** 0.001
- **Batch Size:** 32
- **Epochs:** 50
- **Train/Test Split:** 80/20
- **Device:** CUDA (RTX 4090)

## ğŸ“Š Data Sources

- **H5 Files:** `/root/akhil/HALP_EACL_Models/Models/Smol_VL/smolvlm_output/`
  - 10 files: `smolvlm_2.2b_embeddings_part_001.h5` to `part_010.h5`
  - Total: 10,000 samples

- **Labels:** `/root/akhil/FInal_CSV_Hallucination/smolvlm_manually_reviewed.csv`
  - Total: 10,000 samples
  - No Hallucination: 9,009 (90.1%)
  - Hallucination: 991 (9.9%)

## ğŸš€ How to Run

### Option 1: Run All Probes Sequentially (Recommended)

```bash
# Verify setup first
python3 verify_setup.py

# Run all 11 probes
python3 run_all_probes.py
```

### Option 2: Run Individual Probes

```bash
# Run specific probe
python3 01_vision_only_probe.py
python3 02_vision_token_layer0_probe.py
# ... etc
```

### Option 3: Run with Bash Script

```bash
./run_all_probes.sh
```

## ğŸ“ˆ Expected Outputs

Each probe creates a results directory:

```
results/
â”œâ”€â”€ vision_only/
â”‚   â”œâ”€â”€ probe_model.pt              # Trained model
â”‚   â”œâ”€â”€ results.json                # Metrics (AUROC, F1, etc.)
â”‚   â”œâ”€â”€ confusion_matrix.png        # Confusion matrices
â”‚   â”œâ”€â”€ roc_curve.png              # ROC curves
â”‚   â”œâ”€â”€ training_history.png       # Training curves
â”‚   â””â”€â”€ training_YYYYMMDD_HHMMSS.log
â”‚
â”œâ”€â”€ vision_token_layer0/
â”œâ”€â”€ vision_token_layer6/
â”œâ”€â”€ vision_token_layer12/
â”œâ”€â”€ vision_token_layer18/
â”œâ”€â”€ vision_token_layer23/
â”‚
â”œâ”€â”€ query_token_layer0/
â”œâ”€â”€ query_token_layer6/
â”œâ”€â”€ query_token_layer12/
â”œâ”€â”€ query_token_layer18/
â””â”€â”€ query_token_layer23/
```

## ğŸ“‹ Key Metrics

Each `results.json` contains:

```json
{
  "model_name": "SmolVLM2-2.2B",
  "embedding_type": "vision_token_representation",
  "layer_name": "layer_23",
  "input_dim": 2048,

  "test_set": {
    "accuracy": 0.xxxx,
    "precision": 0.xxxx,
    "recall": 0.xxxx,
    "f1": 0.xxxx,
    "auroc": 0.xxxx
  }
}
```

## â±ï¸ Estimated Runtime

- **Per Probe:** ~5-10 minutes (50 epochs)
- **All 11 Probes:** ~1-2 hours total

## âœ… Verification

Before running, verify setup:

```bash
python3 verify_setup.py
```

This checks:
- âœ“ CSV file exists with correct columns
- âœ“ H5 files exist with correct structure
- âœ“ All dependencies installed
- âœ“ CUDA available

## ğŸ”§ Troubleshooting

**Issue:** CUDA out of memory
**Solution:** Reduce `BATCH_SIZE` in probe scripts (default: 32)

**Issue:** H5 files not found
**Solution:** Verify extraction completed: `ls /root/akhil/HALP_EACL_Models/Models/Smol_VL/smolvlm_output/`

**Issue:** Missing CSV columns
**Solution:** Check CSV has: `question_id`, `image_id`, `question`, `is_hallucinating_manual`

## ğŸ“ Notes

- All probes use the same random seed (42) for reproducibility
- Training uses stratified split to maintain class balance
- Results are saved after each probe completes
- Can interrupt and resume individual probes

## ğŸ“ Model Architecture Details

**SmolVLM2-2.2B:**
- Vision Encoder: 27 layers (1152-dim)
- Text Decoder: 24 layers (2048-dim)
- Layer Selection Strategy: [0, n/4, n/2, 3n/4, n-1]
- Extraction Points:
  - Vision token: After last image token
  - Query token: After last question token

## ğŸ“Š Expected Results

Based on other VLM models, expected test AUROC ranges:
- Vision Only: 0.50 - 0.70
- Early Layers (0, 6): 0.60 - 0.80
- Middle Layers (12): 0.70 - 0.85
- Late Layers (18, 23): 0.80 - 0.95

Query tokens typically perform better than vision tokens in later layers.
