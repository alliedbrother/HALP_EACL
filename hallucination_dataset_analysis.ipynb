{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Dataset Analysis\n",
    "\n",
    "This notebook loads and analyzes hallucination datasets from three Vision-Language Models:\n",
    "- Gemma 3 12B\n",
    "- Qwen2.5-VL-7B\n",
    "- Molmo-7B-O-0924\n",
    "\n",
    "Each dataset contains 10,000 VQA samples with model-generated answers and ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "✓ Gemma 3 12B: 10,000 samples\n",
      "✓ Qwen2.5-VL-7B: 10,000 samples\n",
      "✓ Molmo-7B-O-0924: 10,000 samples\n",
      "✓ original_df: 10,000 samples\n",
      "\n",
      "✅ All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "base_path = Path('/Users/saiakhil/Documents/Thesis/HALP_EACL')\n",
    "\n",
    "gemma_path = base_path / 'Gemma_3' / 'gemma3_hallucination_dataset.csv'\n",
    "qwen_path = base_path / 'Qwen25_VL' / 'qwen25vl_hallucination_dataset.csv'\n",
    "molmo_path = base_path / 'Molmo_V1' / 'molmo_hallucination_dataset.csv'\n",
    "original_path = \"/Users/saiakhil/Documents/Thesis/HALP_EACL/Datasets/final_data/sampled_10k_relational_dataset.csv\"\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "gemma_df = pd.read_csv(gemma_path)\n",
    "print(f\"✓ Gemma 3 12B: {len(gemma_df):,} samples\")\n",
    "\n",
    "qwen_df = pd.read_csv(qwen_path)\n",
    "print(f\"✓ Qwen2.5-VL-7B: {len(qwen_df):,} samples\")\n",
    "\n",
    "molmo_df = pd.read_csv(molmo_path)\n",
    "print(f\"✓ Molmo-7B-O-0924: {len(molmo_df):,} samples\")\n",
    " \n",
    "\n",
    "ori_df = pd.read_csv(original_path)\n",
    "print(f\"✓ original_df: {len(ori_df):,} samples\")\n",
    "\n",
    "print(\"\\n✅ All datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth_answer</th>\n",
       "      <th>model_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>question_comb_1</td>\n",
       "      <td>haloquest_2082.png</td>\n",
       "      <td>How many sharks are present in the travel brochure?</td>\n",
       "      <td>There are no sharks in the brochure ; The travel brochure is without sharks ; There are zero sha...</td>\n",
       "      <td>Based on the image, there are **no sharks** visible in the travel brochure. \\n\\nThe water appear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>question_comb_10</td>\n",
       "      <td>AMBER_348.jpg</td>\n",
       "      <td>Is the forest withering in this image?</td>\n",
       "      <td>n, o</td>\n",
       "      <td>Let's analyze the image to determine if the forest is withering.\\n\\n**Observations:**\\n\\n*   **T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>question_comb_100</td>\n",
       "      <td>haloquest_1937.png</td>\n",
       "      <td>What does Donald Trump's hat say?</td>\n",
       "      <td>He does not have a hat; He is without a hat; A hat is missing from the image</td>\n",
       "      <td>Based on the image, Donald Trump's hat appears to say \"**Make America Great Again**\". It's the i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         question_id            image_id  \\\n",
       "0    question_comb_1  haloquest_2082.png   \n",
       "1   question_comb_10       AMBER_348.jpg   \n",
       "2  question_comb_100  haloquest_1937.png   \n",
       "\n",
       "                                              question  \\\n",
       "0  How many sharks are present in the travel brochure?   \n",
       "1               Is the forest withering in this image?   \n",
       "2                    What does Donald Trump's hat say?   \n",
       "\n",
       "                                                                                   ground_truth_answer  \\\n",
       "0  There are no sharks in the brochure ; The travel brochure is without sharks ; There are zero sha...   \n",
       "1                                                                                                 n, o   \n",
       "2                         He does not have a hat; He is without a hat; A hat is missing from the image   \n",
       "\n",
       "                                                                                          model_answer  \n",
       "0  Based on the image, there are **no sharks** visible in the travel brochure. \\n\\nThe water appear...  \n",
       "1  Let's analyze the image to determine if the forest is withering.\\n\\n**Observations:**\\n\\n*   **T...  \n",
       "2  Based on the image, Donald Trump's hat appears to say \"**Make America Great Again**\". It's the i...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_id', 'image_id', 'question', 'ground_truth_answer',\n",
       "       'model_answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names\n",
    "print(\"Column Names:\")\n",
    "print(gemma_df.columns.tolist())\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of each dataset\n",
    "print(\"\\n📊 GEMMA 3 12B - First 3 Samples\")\n",
    "print(\"=\"*80)\n",
    "display(gemma_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📊 QWEN2.5-VL-7B - First 3 Samples\")\n",
    "print(\"=\"*80)\n",
    "display(qwen_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📊 MOLMO-7B - First 3 Samples\")\n",
    "print(\"=\"*80)\n",
    "display(molmo_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "stats_data = {\n",
    "    'Model': ['Gemma 3 12B', 'Qwen2.5-VL-7B', 'Molmo-7B'],\n",
    "    'Total Samples': [len(gemma_df), len(qwen_df), len(molmo_df)],\n",
    "    'Unique Images': [\n",
    "        gemma_df['image_id'].nunique(),\n",
    "        qwen_df['image_id'].nunique(),\n",
    "        molmo_df['image_id'].nunique()\n",
    "    ],\n",
    "    'Unique Questions': [\n",
    "        gemma_df['question_id'].nunique(),\n",
    "        qwen_df['question_id'].nunique(),\n",
    "        molmo_df['question_id'].nunique()\n",
    "    ],\n",
    "    'Avg Answer Length': [\n",
    "        gemma_df['model_answer'].str.len().mean(),\n",
    "        qwen_df['model_answer'].str.len().mean(),\n",
    "        molmo_df['model_answer'].str.len().mean()\n",
    "    ]\n",
    "}\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "print(\"\\n📈 Dataset Statistics\")\n",
    "print(\"=\"*80)\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Answer Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate answer lengths\n",
    "gemma_df['answer_length'] = gemma_df['model_answer'].str.len()\n",
    "qwen_df['answer_length'] = qwen_df['model_answer'].str.len()\n",
    "molmo_df['answer_length'] = molmo_df['model_answer'].str.len()\n",
    "gemma_df['gt_length'] = gemma_df['ground_truth_answer'].str.len()\n",
    "\n",
    "# Plot answer length distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].hist(gemma_df['answer_length'], bins=50, color='#1f77b4', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Gemma 3 12B - Answer Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Answer Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(gemma_df['answer_length'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {gemma_df[\"answer_length\"].mean():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(qwen_df['answer_length'], bins=50, color='#ff7f0e', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Qwen2.5-VL-7B - Answer Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Answer Length (characters)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(qwen_df['answer_length'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {qwen_df[\"answer_length\"].mean():.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].hist(molmo_df['answer_length'], bins=50, color='#2ca02c', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_title('Molmo-7B - Answer Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Answer Length (characters)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].axvline(molmo_df['answer_length'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {molmo_df[\"answer_length\"].mean():.0f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "data_to_plot = [\n",
    "    gemma_df['answer_length'],\n",
    "    qwen_df['answer_length'],\n",
    "    molmo_df['answer_length']\n",
    "]\n",
    "\n",
    "bp = ax.boxplot(data_to_plot, labels=['Gemma 3 12B', 'Qwen2.5-VL-7B', 'Molmo-7B'],\n",
    "                patch_artist=True, showmeans=True)\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_title('Answer Length Comparison Across Models', fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel('Answer Length (characters)', fontsize=12)\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Answers for Same Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common question IDs\n",
    "common_ids = set(gemma_df['question_id']) & set(qwen_df['question_id']) & set(molmo_df['question_id'])\n",
    "print(f\"Common questions across all models: {len(common_ids):,}\")\n",
    "\n",
    "# Create merged dataset\n",
    "merged_df = gemma_df[['question_id', 'image_id', 'question', 'ground_truth_answer', 'model_answer']].copy()\n",
    "merged_df = merged_df.rename(columns={'model_answer': 'gemma_answer'})\n",
    "\n",
    "qwen_temp = qwen_df[['question_id', 'model_answer']].copy()\n",
    "qwen_temp = qwen_temp.rename(columns={'model_answer': 'qwen_answer'})\n",
    "\n",
    "molmo_temp = molmo_df[['question_id', 'model_answer']].copy()\n",
    "molmo_temp = molmo_temp.rename(columns={'model_answer': 'molmo_answer'})\n",
    "\n",
    "merged_df = merged_df.merge(qwen_temp, on='question_id', how='inner')\n",
    "merged_df = merged_df.merge(molmo_temp, on='question_id', how='inner')\n",
    "\n",
    "print(f\"\\n✓ Merged dataset created with {len(merged_df):,} samples\")\n",
    "print(\"\\nColumns:\", merged_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display a comparison\n",
    "def display_comparison(idx):\n",
    "    row = merged_df.iloc[idx]\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(f\"SAMPLE {idx + 1}\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n📷 Image: {row['image_id']}\")\n",
    "    print(f\"📝 Question ID: {row['question_id']}\")\n",
    "    print(f\"\\n❓ Question:\\n   {row['question']}\")\n",
    "    print(f\"\\n✅ Ground Truth:\\n   {row['ground_truth_answer']}\")\n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(\"\\n🤖 Gemma 3 12B:\")\n",
    "    print(f\"   {row['gemma_answer']}\")\n",
    "    print(\"\\n🤖 Qwen2.5-VL-7B:\")\n",
    "    print(f\"   {row['qwen_answer']}\")\n",
    "    print(\"\\n🤖 Molmo-7B:\")\n",
    "    print(f\"   {row['molmo_answer']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Display first 5 samples\n",
    "for i in range(5):\n",
    "    display_comparison(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Random Sample Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample (run this cell multiple times to see different samples)\n",
    "import random\n",
    "\n",
    "random_idx = random.randint(0, len(merged_df) - 1)\n",
    "display_comparison(random_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Search for Specific Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for questions containing specific keywords\n",
    "search_term = \"shark\"  # Change this to search for different terms\n",
    "\n",
    "matches = merged_df[merged_df['question'].str.contains(search_term, case=False, na=False)]\n",
    "print(f\"Found {len(matches)} questions containing '{search_term}'\\n\")\n",
    "\n",
    "if len(matches) > 0:\n",
    "    # Display first match\n",
    "    idx = matches.index[0]\n",
    "    row_idx = merged_df.index.get_loc(idx)\n",
    "    display_comparison(row_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged dataset to CSV\n",
    "output_path = base_path / 'merged_hallucination_dataset.csv'\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Merged dataset saved to: {output_path}\")\n",
    "print(f\"   Total samples: {len(merged_df):,}\")\n",
    "print(f\"   Columns: {len(merged_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Samples',\n",
    "        'Mean Answer Length',\n",
    "        'Median Answer Length',\n",
    "        'Min Answer Length',\n",
    "        'Max Answer Length',\n",
    "        'Std Answer Length'\n",
    "    ],\n",
    "    'Gemma 3 12B': [\n",
    "        len(gemma_df),\n",
    "        f\"{gemma_df['answer_length'].mean():.1f}\",\n",
    "        f\"{gemma_df['answer_length'].median():.1f}\",\n",
    "        f\"{gemma_df['answer_length'].min():.0f}\",\n",
    "        f\"{gemma_df['answer_length'].max():.0f}\",\n",
    "        f\"{gemma_df['answer_length'].std():.1f}\"\n",
    "    ],\n",
    "    'Qwen2.5-VL-7B': [\n",
    "        len(qwen_df),\n",
    "        f\"{qwen_df['answer_length'].mean():.1f}\",\n",
    "        f\"{qwen_df['answer_length'].median():.1f}\",\n",
    "        f\"{qwen_df['answer_length'].min():.0f}\",\n",
    "        f\"{qwen_df['answer_length'].max():.0f}\",\n",
    "        f\"{qwen_df['answer_length'].std():.1f}\"\n",
    "    ],\n",
    "    'Molmo-7B': [\n",
    "        len(molmo_df),\n",
    "        f\"{molmo_df['answer_length'].mean():.1f}\",\n",
    "        f\"{molmo_df['answer_length'].median():.1f}\",\n",
    "        f\"{molmo_df['answer_length'].min():.0f}\",\n",
    "        f\"{molmo_df['answer_length'].max():.0f}\",\n",
    "        f\"{molmo_df['answer_length'].std():.1f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n📊 Comprehensive Summary Statistics\")\n",
    "print(\"=\"*80)\n",
    "display(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\\n\")\n",
    "print(\"Gemma 3:\")\n",
    "print(gemma_df.isnull().sum())\n",
    "print(\"\\nQwen2.5-VL:\")\n",
    "print(qwen_df.isnull().sum())\n",
    "print(\"\\nMolmo-7B:\")\n",
    "print(molmo_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate question IDs\n",
    "print(\"Duplicate Question IDs:\\n\")\n",
    "print(f\"Gemma 3: {gemma_df['question_id'].duplicated().sum()}\")\n",
    "print(f\"Qwen2.5-VL: {qwen_df['question_id'].duplicated().sum()}\")\n",
    "print(f\"Molmo-7B: {molmo_df['question_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Quick Access to Individual Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick access variables\n",
    "print(\"Available DataFrames:\")\n",
    "print(\"  - gemma_df: Gemma 3 12B dataset\")\n",
    "print(\"  - qwen_df: Qwen2.5-VL-7B dataset\")\n",
    "print(\"  - molmo_df: Molmo-7B dataset\")\n",
    "print(\"  - merged_df: Merged dataset with all three models\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  gemma_df.head()\")\n",
    "print(\"  merged_df[merged_df['question'].str.contains('color')]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "halp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
